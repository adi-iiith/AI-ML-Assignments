{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "This is a walkthrough of some code for a simple decision-tree learner.\n",
    "\n",
    "We'll import a dataset, where each row is a mushroom, and each column specifies attributes of that mushroom. We want to guess whether the mushroom is poisonous or not based on the other attributes.\n",
    "\n",
    "NOTE: This is not a robust version of the algorithm: for example, it can only handle discrete/nominal attributes, it doesn't handle overfitting, etc. The purpose is for the code to be relatively simple and easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../datasets/mushroom_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-04c82432bb83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_shroom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../datasets/mushroom_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_shroom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[0;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m   1577\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../datasets/mushroom_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "df_shroom = DataFrame.from_csv('../datasets/mushroom_data.csv')\n",
    "df_shroom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "The decision tree algorithm works by looking at all the attributes, and picking the 'best' one to split the data on, then running recursively on the split data.\n",
    "\n",
    "The way we pick the 'best' attribute to split on is by picking the attribute that decreases 'entropy'.\n",
    "\n",
    "What's entropy? Recall that we're trying to classify mushrooms as poisonous or not. Entropy is a value that will be low if a group of mushrooms mostly has the same class (all poisonous or all edible) and high if a group of mushrooms varies in their classes (half poisonous and half edible). So every split of the data that minimizes entropy is a split that does a good job classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_shroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-beb1ab3257a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# The initial entropy of the poisonous/not attribute for our dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtotal_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy_of_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_shroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# print total_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_shroom' is not defined"
     ]
    }
   ],
   "source": [
    "def entropy(probs):\n",
    "    '''\n",
    "    Takes a list of probabilities and calculates their entropy\n",
    "    '''\n",
    "    import math\n",
    "    return sum( [-prob*math.log(prob, 2) for prob in probs] )\n",
    "    \n",
    "\n",
    "def entropy_of_list(a_list):\n",
    "    '''\n",
    "    Takes a list of items with discrete values (e.g., poisonous, edible)\n",
    "    and returns the entropy for those items.\n",
    "    '''\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Tally Up:\n",
    "    cnt = Counter(x for x in a_list)\n",
    "    print(cnt)\n",
    "    # Convert to Proportion\n",
    "    num_instances = len(a_list)*1.0\n",
    "    probs = [x / num_instances for x in cnt.values()]\n",
    "    \n",
    "    # Calculate Entropy:\n",
    "    return entropy(probs)\n",
    "    \n",
    "# The initial entropy of the poisonous/not attribute for our dataset.\n",
    "total_entropy = entropy_of_list(df_shroom['class'])\n",
    "# print total_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide which attribute to split on, we want to quantify how each attribute decreases the entropy. \n",
    "\n",
    "We do this in a fairly intuitive way: we split our dataset by the possible values of an attribute, then do a weighted sum of the entropies for each of these split datasets, weighted by how big that sub-dataset is. \n",
    "\n",
    "We'll make a function that quantifies the decrease in entropy, or conversely, the gain in information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example: Info-gain for best attribute is 0.859670435885\n"
     ]
    }
   ],
   "source": [
    "def information_gain(df, split_attribute_name, target_attribute_name, trace=0):\n",
    "    '''\n",
    "    Takes a DataFrame of attributes, and quantifies the entropy of a target\n",
    "    attribute after performing a split along the values of another attribute.\n",
    "    '''\n",
    "    \n",
    "    # Split Data by Possible Vals of Attribute:\n",
    "    df_split = df.groupby(split_attribute_name)\n",
    "    \n",
    "    # Calculate Entropy for Target Attribute, as well as Proportion of Obs in Each Data-Split\n",
    "    nobs = len(df.index) * 1.0\n",
    "    df_agg_ent = df_split.agg({target_attribute_name : [entropy_of_list, lambda x: len(x)/nobs] })[target_attribute_name]\n",
    "    df_agg_ent.columns = ['Entropy', 'PropObservations']\n",
    "    if trace: # helps understand what fxn is doing:\n",
    "        print df_agg_ent\n",
    "    \n",
    "    # Calculate Information Gain:\n",
    "    new_entropy = sum( df_agg_ent['Entropy'] * df_agg_ent['PropObservations'] )\n",
    "    old_entropy = entropy_of_list(df[target_attribute_name])\n",
    "    return old_entropy-new_entropy\n",
    "\n",
    "print '\\nExample: Info-gain for best attribute is ' + str( information_gain(df_shroom, 'odor', 'class') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Decision Tree Algorithm\n",
    "\n",
    "Now we'll write the decision tree algorithm itself, which is called \"ID3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(df, target_attribute_name, attribute_names, default_class=None):\n",
    "    \n",
    "    ## Tally target attribute:\n",
    "    from collections import Counter\n",
    "    cnt = Counter(x for x in df[target_attribute_name])\n",
    "    \n",
    "    ## First check: Is this split of the dataset homogeneous?\n",
    "    # (e.g., all mushrooms in this set are poisonous)\n",
    "    # if yes, return that homogenous label (e.g., 'poisonous')\n",
    "    if len(cnt) == 1:\n",
    "        return cnt.keys()[0]\n",
    "    \n",
    "    ## Second check: Is this split of the dataset empty?\n",
    "    # if yes, return a default value\n",
    "    elif df.empty or (not attribute_names):\n",
    "        return default_class \n",
    "    \n",
    "    ## Otherwise: This dataset is ready to be divvied up!\n",
    "    else:\n",
    "        # Get Default Value for next recursive call of this function:\n",
    "        index_of_max = cnt.values().index(max(cnt.values())) \n",
    "        default_class = cnt.keys()[index_of_max] # most common value of target attribute in dataset\n",
    "        \n",
    "        # Choose Best Attribute to split on:\n",
    "        gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names]\n",
    "        index_of_max = gainz.index(max(gainz)) \n",
    "        best_attr = attribute_names[index_of_max]\n",
    "        \n",
    "        # Create an empty tree, to be populated in a moment\n",
    "        tree = {best_attr:{}}\n",
    "        remaining_attribute_names = [i for i in attribute_names if i != best_attr]\n",
    "        \n",
    "        # Split dataset\n",
    "        # On each split, recursively call this algorithm.\n",
    "        # populate the empty tree with subtrees, which\n",
    "        # are the result of the recursive call\n",
    "        for attr_val, data_subset in df.groupby(best_attr):\n",
    "            subtree = id3(data_subset,\n",
    "                        target_attribute_name,\n",
    "                        remaining_attribute_names,\n",
    "                        default_class)\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Predictor Names (all but 'class')\n",
    "attribute_names = list(df_shroom.columns)\n",
    "attribute_names.remove('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'odor': {'almond': 'edible',\n",
      "          'anise': 'edible',\n",
      "          'creosote': 'poisonous',\n",
      "          'foul': 'poisonous',\n",
      "          'musty': 'poisonous',\n",
      "          'none': {'spore-print-color': {'black': 'edible',\n",
      "                                         'brown': 'edible',\n",
      "                                         'green': 'poisonous',\n",
      "                                         'white': {'ring-type': {'evanescent': {'stalk-surface-above-ring': {'fibrous': 'edible',\n",
      "                                                                                                             'scaly': 'poisonous',\n",
      "                                                                                                             'smooth': 'edible'}},\n",
      "                                                                 'pendant': {'stalk-surface-above-ring': {'scaly': 'edible',\n",
      "                                                                                                          'smooth': {'stalk-surface-below-ring': {'smooth': {'stalk-color-above-ring': {'white': {'stalk-color-below-ring': {'white': {'stalk-shape': {'enlarging': {'gill-color': {'white': {'cap-color': {'brown': 'edible',\n",
      "                                                                                                                                                                                                                                                                                                            'cinnamon': 'edible',\n",
      "                                                                                                                                                                                                                                                                                                            'gray': 'edible',\n",
      "                                                                                                                                                                                                                                                                                                            'pink': 'edible',\n",
      "                                                                                                                                                                                                                                                                                                            'white': 'poisonous'}}}}}}}}}}}}}}}}}},\n",
      "          'pungent': 'poisonous'}}\n"
     ]
    }
   ],
   "source": [
    "# Run Algorithm:\n",
    "from pprint import pprint\n",
    "tree = id3(df_shroom, 'class', attribute_names)\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy\n",
    "\n",
    "Let's make sure our resulting tree accurately predicts the class, based on the features.\n",
    "\n",
    "Below is a 'classify' algorithm that takes an instance and classifies it based on the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(instance, tree, default=None):\n",
    "    attribute = tree.keys()[0]\n",
    "    if instance[attribute] in tree[attribute].keys():\n",
    "        result = tree[attribute][instance[attribute]]\n",
    "        if isinstance(result, dict): # this is a tree, delve deeper\n",
    "            return classify(instance, result)\n",
    "        else:\n",
    "            return result # this is a label\n",
    "    else:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0   </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1   </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2   </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3   </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4   </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5   </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6   </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7   </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8   </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9   </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13  </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17  </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18  </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19  </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21  </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25  </th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29  </th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5614</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5615</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5616</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5623</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5628</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5636</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5637</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5638</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>    edible</td>\n",
       "      <td>    edible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td> poisonous</td>\n",
       "      <td> poisonous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          class  predicted\n",
       "0     poisonous  poisonous\n",
       "1        edible     edible\n",
       "2        edible     edible\n",
       "3     poisonous  poisonous\n",
       "4        edible     edible\n",
       "5        edible     edible\n",
       "6        edible     edible\n",
       "7        edible     edible\n",
       "8     poisonous  poisonous\n",
       "9        edible     edible\n",
       "10       edible     edible\n",
       "11       edible     edible\n",
       "12       edible     edible\n",
       "13    poisonous  poisonous\n",
       "14       edible     edible\n",
       "15       edible     edible\n",
       "16       edible     edible\n",
       "17    poisonous  poisonous\n",
       "18    poisonous  poisonous\n",
       "19    poisonous  poisonous\n",
       "20       edible     edible\n",
       "21    poisonous  poisonous\n",
       "22       edible     edible\n",
       "23       edible     edible\n",
       "24       edible     edible\n",
       "25    poisonous  poisonous\n",
       "26       edible     edible\n",
       "27       edible     edible\n",
       "28       edible     edible\n",
       "29       edible     edible\n",
       "...         ...        ...\n",
       "5614     edible     edible\n",
       "5615     edible     edible\n",
       "5616  poisonous  poisonous\n",
       "5617  poisonous  poisonous\n",
       "5618  poisonous  poisonous\n",
       "5619     edible     edible\n",
       "5620     edible     edible\n",
       "5621  poisonous  poisonous\n",
       "5622  poisonous  poisonous\n",
       "5623     edible     edible\n",
       "5624  poisonous  poisonous\n",
       "5625  poisonous  poisonous\n",
       "5626  poisonous  poisonous\n",
       "5627     edible     edible\n",
       "5628     edible     edible\n",
       "5629  poisonous  poisonous\n",
       "5630     edible     edible\n",
       "5631     edible     edible\n",
       "5632     edible     edible\n",
       "5633  poisonous  poisonous\n",
       "5634     edible     edible\n",
       "5635     edible     edible\n",
       "5636     edible     edible\n",
       "5637  poisonous  poisonous\n",
       "5638     edible     edible\n",
       "5639     edible     edible\n",
       "5640     edible     edible\n",
       "5641     edible     edible\n",
       "5642  poisonous  poisonous\n",
       "5643  poisonous  poisonous\n",
       "\n",
       "[5644 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shroom['predicted'] = df_shroom.apply(classify, axis=1, args=(tree,'poisonous') ) \n",
    "    # classify func allows for a default arg: when tree doesn't have answer for a particular\n",
    "    # combitation of attribute-values, we can use 'poisonous' as the default guess (better safe than sorry!)\n",
    "\n",
    "print 'Accuracy is ' + str( sum(df_shroom['class']==df_shroom['predicted'] ) / (1.0*len(df_shroom.index)) )\n",
    "\n",
    "df_shroom[['class', 'predicted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy: Training/Testing Set\n",
    "\n",
    "Of course, a more accurate assessement of the algorithm is to train it on a subset of the data, then test it on a different subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.944\n"
     ]
    }
   ],
   "source": [
    "training_data = df_shroom.iloc[1:-1000] # all but last thousand instances\n",
    "test_data  = df_shroom.iloc[-1000:] # just the last thousand\n",
    "train_tree = id3(training_data, 'class', attribute_names)\n",
    "\n",
    "test_data['predicted2'] = test_data.apply(                                # <---- test_data source\n",
    "                                          classify, \n",
    "                                          axis=1, \n",
    "                                          args=(train_tree,'poisonous') ) # <---- train_data tree\n",
    "\n",
    "print 'Accuracy is ' + str( sum(test_data['class']==test_data['predicted2'] ) / (1.0*len(test_data.index)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different source\n",
    "\n",
    "\"\"\"\n",
    "Make the imports of python packages needed\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "#Import the dataset and define the feature as well as the target datasets / columns#\n",
    "dataset = pd.read_csv('data/zoo.csv',\n",
    "                      names=['animal_name','hair','feathers','eggs','milk',\n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])#Import all columns omitting the fist which consists the names of the animals\n",
    "#We drop the animal names since this is not a good feature to split the data on\n",
    "dataset=dataset.drop('animal_name',axis=1)\n",
    "###########################################################################################################\n",
    "def entropy(target_col):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "    The only parameter of this function is the target_col parameter which specifies the target column\n",
    "    \"\"\"\n",
    "    elements,counts = np.unique(target_col,return_counts = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "########################################################################################################### \n",
    "    \n",
    "###########################################################################################################\n",
    "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a dataset. This function takes three parameters:\n",
    "    1. data = The dataset for whose feature the IG should be calculated\n",
    "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
    "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
    "    \"\"\"    \n",
    "    #Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    ##Calculate the entropy of the dataset\n",
    "    \n",
    "    #Calculate the values and the corresponding counts for the split attribute \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    #Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain\n",
    "       \n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):\n",
    "    \"\"\"\n",
    "    ID3 Algorithm: This function takes five paramters:\n",
    "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
    " \n",
    "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
    "    in the case the dataset delivered by the first parameter is empty\n",
    "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
    "    we have to remove features from our dataset --> Splitting at each node\n",
    "    4. target_attribute_name = the name of the target attribute\n",
    "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
    "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
    "    space, we want to return the mode target feature value of the direct parent node.\n",
    "    \"\"\"   \n",
    "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
    "    \n",
    "    #If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "    \n",
    "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
    "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
    "    #the mode target feature value is stored in the parent_node_class variable.\n",
    "    \n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    #If none of the above holds true, grow the tree!\n",
    "    \n",
    "    else:\n",
    "        #Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #Select the feature which best splits the dataset\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
    "        #gain in the first run\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        #Remove the feature with the best inforamtion gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "                \n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "    \n",
    "    \n",
    "def predict(query,tree,default = 1):\n",
    "    \"\"\"\n",
    "    Prediction of a new/unseen query instance. This takes two parameters:\n",
    "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
    "    2. The tree \n",
    "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
    "    Since this is a important step to understand, the single steps are extensively commented below.\n",
    "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
    "    tree.keys() only contains the value for the root node \n",
    "    --> if this value is not existing, we can not make a prediction and have to \n",
    "    return the default value which is the majority value of the target feature\n",
    "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
    "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
    "    training instances has had such a value for this specific feature. \n",
    "    For instance imagine the situation where your model has only seen animals with one to four\n",
    "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
    "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
    "    situation because otherwise there is no classification possible because in the classification step you try to \n",
    "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
    "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
    "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
    "    medical application we can return the most worse case - just to make sure... \n",
    "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
    "    what to do in this situation.\n",
    "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
    "    the value 1 which is the value for the mammal species (for convenience).\n",
    "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
    "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
    "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
    "    - What are you doing? - Correct:\n",
    "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
    "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
    "    get to a leaf node. \n",
    "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
    "    and hence we must address the node in the tree which == the key value from our query instance. \n",
    "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
    "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
    "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
    "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
    "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
    "    this is done with: result = [key][query[key]]\n",
    "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
    "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
    "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
    "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
    "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
    "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
    "    under 'result'.\n",
    "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
    "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
    "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
    "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
    "    name of the root node is equal to one of the query features.\n",
    "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
    "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
    "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
    "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
    "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
    "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
    "    below this node and do not run the whole tree beginning at the root node \n",
    "    --> This is why we re-call the classification function with 'result'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #1.\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            #2.\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "  \n",
    "            #3.\n",
    "            result = tree[key][query[key]]\n",
    "            #4.\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "            else:\n",
    "                return result\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "Check the accuracy of our prediction.\n",
    "The train_test_split function takes the dataset as parameter which should be divided into\n",
    "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
    "\"\"\"\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "def train_test_split(dataset):\n",
    "    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index\n",
    "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
    "    testing_data = dataset.iloc[80:].reset_index(drop=True)\n",
    "    return training_data,testing_data\n",
    "training_data = train_test_split(dataset)[0]\n",
    "testing_data = train_test_split(dataset)[1] \n",
    "def test(data,tree):\n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "    #convert it to a dictionary\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    \n",
    "    #Calculate the prediction accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
    "    \n",
    "\"\"\"\n",
    "Train the tree, Print the tree and predict the accuracy\n",
    "\"\"\"\n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
    "pprint(tree)\n",
    "test(testing_data,tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
