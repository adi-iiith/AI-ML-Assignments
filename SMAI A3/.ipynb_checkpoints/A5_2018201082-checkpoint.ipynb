{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 1\n",
    "    \n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/data.csv\")\n",
    "# lab = df['label']\n",
    "actual = df['xAttack']\n",
    "df = df.drop(labels = 'xAttack', axis = 1)\n",
    "df = (df - df.mean())/df.std()\n",
    "\n",
    "# df.insert(0,'label',lab,allow_duplicates=False)\n",
    "train = df\n",
    "validate = df\n",
    "\n",
    "train = np.array(train)\n",
    "train = train.T\n",
    "\n",
    "validate = np.array(validate)\n",
    "validate = validate.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ones = []\n",
    "# for i in range(train_act.shape[0]) :\n",
    "#     temp = [0 for x in range(10)]\n",
    "#     temp[train_act[i][0]] = 1\n",
    "#     ones.append(temp)\n",
    "\n",
    "# y_train_encoded = np.array(ones)\n",
    "# y_train_encoded = y_train_encoded.T\n",
    "\n",
    "# print(y_train_encoded.shape)\n",
    "\n",
    "batched_train_out = {}\n",
    "batched_train = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(train, batchsize, no_of_batches) :\n",
    "    global batched_train\n",
    "    global batched_train_out\n",
    "    temp = 0\n",
    "    for i in range(no_of_batches) :\n",
    "        batched_train[i] = train[:,temp*batchsize:temp*batchsize + batchsize]\n",
    "        batched_train_out[i] = batched_train[i]\n",
    "        temp+=1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x) :\n",
    "    t = np.exp(x)\n",
    "    return t/ np.sum(t, axis=0)\n",
    "\n",
    "def relu(x) :\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def tanh(x) :\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_sigmoid(x) :\n",
    "    return x * (1-x)\n",
    "\n",
    "def d_relu(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def d_tanh(x) :\n",
    "    return (1 - (x ** 2))\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(weight, bias, batch_no, no_of_layers, out_dict, gr_truth_out, Z, \n",
    "                 hidden_activation, output_activation) :\n",
    "    for i in range(no_of_layers-1) :\n",
    "        if(hidden_activation == 'relu') :\n",
    "            Z[i+1] = np.matmul(weight[i+1],out_dict[i]) + bias[i+1]\n",
    "            out_dict[i+1] = relu(Z[i+1]) \n",
    "        elif(hidden_activation == 'tanh') :\n",
    "            Z[i+1] = np.matmul(weight[i+1],out_dict[i]) + bias[i+1]\n",
    "            out_dict[i+1] = tanh(Z[i+1])\n",
    "        \n",
    "        else:\n",
    "            Z[i+1] = np.matmul(weight[i+1],out_dict[i]) + bias[i+1]\n",
    "            out_dict[i+1] = sigmoid(Z[i+1]) \n",
    "    \n",
    "    if(output_activation == 'sigmoid') :\n",
    "        Z[no_of_layers] = np.matmul(weight[no_of_layers],out_dict[no_of_layers-1]) + bias[no_of_layers]\n",
    "        out_dict[no_of_layers] = sigmoid(Z[no_of_layers])\n",
    "    elif(output_activation == 'relu') :\n",
    "        Z[no_of_layers] = np.matmul(weight[no_of_layers],out_dict[no_of_layers-1]) + bias[no_of_layers]\n",
    "        out_dict[no_of_layers] = relu(Z[no_of_layers])\n",
    "    elif(output_activation == 'linear') :\n",
    "            Z[no_of_layers] = np.matmul(weight[no_of_layers],out_dict[no_of_layers-1]) + bias[no_of_layers]\n",
    "            out_dict[no_of_layers] = (Z[no_of_layers])\n",
    "    elif(output_activation == 'softmax') :\n",
    "        Z[no_of_layers] = np.matmul(weight[no_of_layers],out_dict[no_of_layers-1]) + bias[no_of_layers]\n",
    "        out_dict[no_of_layers] = softmax(Z[no_of_layers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(weight, bias, batch_no, no_of_layers, out_dict, gr_truth_out, D_weight,\n",
    "              D_bias, batchsize, Z, dZ, hidden_activation, output_activation) :\n",
    "    if output_activation == 'linear':\n",
    "        dZ[no_of_layers] = out_dict[no_of_layers] - gr_truth_out\n",
    "        D_weight[no_of_layers] = (1/batchsize) * np.matmul(dZ[no_of_layers], out_dict[no_of_layers-1].T)\n",
    "        D_bias[no_of_layers] = 1/batchsize * np.sum(dZ[no_of_layers], axis = 1, keepdims = True)\n",
    "        weight[no_of_layers]-= 0.0001 * D_weight[no_of_layers]\n",
    "        bias[no_of_layers]-= 0.0001 * D_bias[no_of_layers]\n",
    "        \n",
    "    else:\n",
    "        dZ[no_of_layers] = (out_dict[no_of_layers] - gr_truth_out)*d_relu(out_dict[no_of_layers])\n",
    "        D_weight[no_of_layers] = (1/batchsize) * np.matmul(dZ[no_of_layers], out_dict[no_of_layers-1].T)\n",
    "        D_bias[no_of_layers] = 1/batchsize * np.sum(dZ[no_of_layers], axis = 1, keepdims = True)\n",
    "        weight[no_of_layers]-= 0.0001 * D_weight[no_of_layers]\n",
    "        bias[no_of_layers]-= 0.0001 * D_bias[no_of_layers]\n",
    "        \n",
    "    LR = 0.001\n",
    "    if(hidden_activation == 'relu') :\n",
    "        for i in range(no_of_layers-1,0,-1) :\n",
    "            dZ[i] = np.matmul(weight[i+1].T,dZ[i+1]) * d_relu(Z[i])\n",
    "            D_weight[i] = 1/batchsize * np.matmul(dZ[i],out_dict[i-1].T)\n",
    "\n",
    "            D_bias[i] = 1/batchsize * np.sum(dZ[i], axis = 1, keepdims = True)\n",
    "            weight[i]-= LR * D_weight[i]\n",
    "            bias[i]-= LR * D_bias[i]\n",
    "    elif(hidden_activation == 'tanh') :\n",
    "        for i in range(no_of_layers-1,0,-1) :\n",
    "            dZ[i] = np.matmul(weight[i+1].T,dZ[i+1]) * d_tanh(Z[i])\n",
    "            D_weight[i] = 1/batchsize * np.matmul(dZ[i],out_dict[i-1].T)\n",
    "            D_bias[i] = 1/batchsize * np.sum(dZ[i], axis = 1, keepdims = True)\n",
    "            weight[i]-= LR * D_weight[i]\n",
    "            bias[i]-= LR * D_bias[i]\n",
    "    elif(hidden_activation == 'sigmoid') :\n",
    "        for i in range(no_of_layers-1,0,-1) :\n",
    "            dZ[i] = np.matmul(weight[i+1].T,dZ[i+1]) * d_sigmoid(Z[i])\n",
    "            D_weight[i] = 1/batchsize * np.matmul(dZ[i],out_dict[i-1].T)\n",
    "            D_bias[i] = 1/batchsize * np.sum(dZ[i], axis = 1, keepdims = True)\n",
    "            weight[i]-= LR * D_weight[i]\n",
    "            bias[i]-= LR * D_bias[i]\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter activation function for hidden layers  : relu\n",
      "Enter activation function for hidden layers and output layer : linear\n",
      "Epochs ::   300\n"
     ]
    }
   ],
   "source": [
    "#Structures\n",
    "weight = {}\n",
    "D_weight = {}\n",
    "bias = {}\n",
    "D_bias = {}\n",
    "out_dict = {}\n",
    "gr_truth_out = {}\n",
    "Z = {}\n",
    "dZ = {}\n",
    "\n",
    "no_of_layers = 2\n",
    "batchsize = 32\n",
    "no_of_nodes_layer = {}\n",
    "\n",
    "hidden_activation = input(\"Enter activation function for hidden layers  : \")\n",
    "output_activation = input(\"Enter activation function for hidden layers and output layer : \")\n",
    "\n",
    "test_dict = {}\n",
    "test_z_dict = {}\n",
    "\n",
    "\n",
    "no_of_nodes_layer[0] = 29\n",
    "no_of_nodes_layer[1] = 14\n",
    "no_of_nodes_layer[2] = 29\n",
    "\n",
    "# for i in range(no_of_layers) :\n",
    "#     no_of_nodes_layer[i+1] = int(input(\"Enter the number of nodes for layer  : \"))\n",
    "\n",
    "\n",
    "for i in range(no_of_layers) :\n",
    "    Matrix = [[random.uniform(0.01,0.001) for x in range(no_of_nodes_layer[i])]\n",
    "              for y in range(no_of_nodes_layer[i+1])] \n",
    "    Matrix = np.array(Matrix)\n",
    "    weight[i+1] = Matrix\n",
    "    bias[i+1] = [[random.uniform(0.01,0.001) for j in range(no_of_nodes_layer[i+1])]]\n",
    "    bias[i+1] = np.array(bias[i+1])\n",
    "    bias[i+1] = bias[i+1].T\n",
    "    np.reshape(bias[i+1],no_of_nodes_layer[i+1],1)\n",
    "\n",
    "\n",
    "\n",
    "no_of_batches = math.ceil(train.shape[1]/batchsize)\n",
    "make_batch(train, batchsize, no_of_batches)\n",
    "\n",
    "iterations = 300\n",
    "for j in range(iterations) :\n",
    "    for i in range(no_of_batches) :\n",
    "        out_dict[0] = batched_train[i]\n",
    "        gr_truth_out = batched_train_out[i]\n",
    "        forward_prop(weight, bias, i, no_of_layers, out_dict, gr_truth_out, Z,\n",
    "                     hidden_activation, output_activation)\n",
    "        back_prop(weight, bias, i, no_of_layers, out_dict, gr_truth_out, \n",
    "                  D_weight, D_bias, batchsize, Z,dZ, hidden_activation, output_activation)\n",
    "#     print(j)\n",
    "print(\"Epochs ::  \",iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10285617984266772\n"
     ]
    }
   ],
   "source": [
    "#testing code :\n",
    "# df = df.loc[:,~df.columns.duplicated()]\n",
    "test_dict[0] = validate\n",
    "# print(df.columns)\n",
    "# print(weight[1].shape)\n",
    "# print(test_dict[0].shape)\n",
    "\n",
    "for i in range(no_of_layers) :\n",
    "    test_z_dict[i+1] = np.matmul(weight[i+1],test_dict[i]) + bias[i+1]\n",
    "    test_dict[i+1] = relu(test_z_dict[i+1]) \n",
    "#     print(test_dict[i].shape)\n",
    "    \n",
    "# test_z_dict[no_of_layers] = np.matmul(weight[no_of_layers],test_dict[no_of_layers-1]) + bias[no_of_layers]\n",
    "# test_dict[no_of_layers] = sigmoid(test_z_dict[no_of_layers])\n",
    "\n",
    "# y_out = test_dict[no_of_layers].argmax(axis=0)\n",
    "y_out = test_z_dict[no_of_layers]\n",
    "\n",
    "# y_out = y_out.reshape(12000,1)\n",
    "# validate = df.T\n",
    "# y_out = y_out.T\n",
    "# print(\"Accuracy :: \",metrics.accuracy_score(validate_act,y_out)*100)\n",
    "mse = np.mean(np.square(np.subtract(y_out,validate)))\n",
    "print(mse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = actual.unique()\n",
    "out_dict[0] = validate\n",
    "forward_prop(weight, bias, i, no_of_layers, out_dict, gr_truth_out, Z,hidden_activation, output_activation)\n",
    "# print(out_dict[no_of_layers-1])\n",
    "projections = out_dict[no_of_layers-1]\n",
    "data = copy.deepcopy(projections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Iterations :  2\n",
      "Purity of cluster  1 :  0.6666666666666666\n",
      "Purity of cluster  2 :  0.6666666666666666\n",
      "Purity of cluster  3 :  1.0\n",
      "Purity of cluster  4 :  0.6666666666666666\n",
      "Purity of cluster  5 :  1.0\n",
      "Average Purity of Cluster :  0.7142857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "k_cluster = 5\n",
    "th_count = 999\n",
    "\n",
    "label = {}\n",
    "centroids = {}\n",
    "final_l = {}\n",
    "\n",
    "for i in range(5):\n",
    "    label[output[i]] = i\n",
    "\n",
    "centroids[0] = projections[0]\n",
    "centroids[1] = projections[1]\n",
    "centroids[2] = projections[2]\n",
    "centroids[3] = projections[3]\n",
    "centroids[4] = projections[4]\n",
    "\n",
    "for i in range(th_count):\n",
    "    cluster = {}\n",
    "    final_l.clear()\n",
    "    \n",
    "    for j in range(k_cluster):\n",
    "        cluster[j] = []\n",
    "        final_l[j] = []\n",
    "        \n",
    "        for k in range(k_cluster):\n",
    "            final_l[j].append(0)\n",
    "        \n",
    "    for j,l in zip(projections,range(projections.shape[0])):\n",
    "        dist = []\n",
    "        for k in centroids:\n",
    "            dist.append(np.linalg.norm(j - centroids[k]))\n",
    "        classify = dist.index(min(dist))\n",
    "        cluster[classify].append(j)\n",
    "        final_l[classify][label[actual[l]]]+= 1\n",
    "        \n",
    "    previous_value = {}\n",
    "        \n",
    "    for j in range(k_cluster):\n",
    "        previous_value[j] = centroids[j]\n",
    "        average = np.mean(cluster[j],axis = 0)\n",
    "        centroids[j] = average\n",
    "        \n",
    "    convergence = True\n",
    "        \n",
    "    for j in range(k_cluster):\n",
    "        if np.sum(abs(previous_value[j] - centroids[j])/abs(centroids[j]) * 100) > 0.001:\n",
    "            convergence = False\n",
    "            break\n",
    "    if convergence == True:\n",
    "        break\n",
    "        \n",
    "print('Total Iterations : ',(i+1))\n",
    "purity = 0.0\n",
    "for i in range(k_cluster):\n",
    "    total = np.sum(final_l[i])\n",
    "    max_value = max(final_l[i])\n",
    "    print(\"Purity of cluster \",(i+1),': ',(max_value/total))\n",
    "for i in range(k_cluster):\n",
    "    total = np.sum(final_l[i])\n",
    "    max_value = max(final_l[i])\n",
    "    ppurity = max_value/total\n",
    "    percent = (total*ppurity)/float(len(data))\n",
    "    purity += percent  \n",
    "print(\"Average Purity of Cluster : \",purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part 3 : GMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-48c547a48506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdict_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mGMM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclusters_gmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGMM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clusters_gmm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusters_gmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mavg_purity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfty\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bound_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/mixture/base.py\u001b[0m in \u001b[0;36m_initialize_parameters\u001b[0;34m(self, X, random_state)\u001b[0m\n\u001b[1;32m    156\u001b[0m                              % self.init_params)\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/mixture/gaussian_mixture.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, X, resp)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         weights, means, covariances = _estimate_gaussian_parameters(\n\u001b[0;32m--> 640\u001b[0;31m             X, resp, self.reg_covar, self.covariance_type)\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/mixture/gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_gaussian_parameters\u001b[0;34m(X, resp, reg_covar, covariance_type)\u001b[0m\n\u001b[1;32m    283\u001b[0m                    \u001b[0;34m\"diag\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_estimate_gaussian_covariances_diag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                    \u001b[0;34m\"spherical\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_estimate_gaussian_covariances_spherical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                    }[covariance_type](resp, X, nk, means, reg_covar)\n\u001b[0m\u001b[1;32m    286\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/mixture/gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_gaussian_covariances_full\u001b[0;34m(resp, X, nk, means, reg_covar)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \"\"\"\n\u001b[1;32m    163\u001b[0m     \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mcovariances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "projections = out_dict[no_of_layers-1]\n",
    "dict_max = {}\n",
    "GMM = GaussianMixture(n_components=5)\n",
    "clusters_gmm = GMM.fit_predict(projections)\n",
    "data['clusters_gmm'] = clusters_gmm\n",
    "avg_purity = 0.0\n",
    "for i in range(5):\n",
    "    t = data[data['clusters_gmm'] == i]\n",
    "    dict_max[i] = t.groupby('xAttack')['clusters_gmm'].count().idxmax()\n",
    "    purity_percent = len(t[t['xAttack'] == dict_max[i]])/float(len(t))\n",
    "    avg_purity = avg_purity + (purity_percent * len(t))/len(data)\n",
    "    \n",
    "    print(\"Clusters with label \",dict_max[i] + \" purity_percent :: \",str(purity_percent))\n",
    "\n",
    "print(\"Average purity_percent is :: \",avg_purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7f8cd92177b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'single'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clusters'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0maverage_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "dict_max = {}\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=5,linkage='single')\n",
    "clusters = clustering.fit_predict(projections)\n",
    "data['clusters'] = clusters\n",
    "average_agg = 0.0\n",
    "for i in range(5):\n",
    "    t = data[data['clusters'] == i]\n",
    "    dict_max[i] = t.groupby('xAttack')['clusters'].count().idxmax()\n",
    "    percent = len(t[t['xAttack'] == dict_max[i]])/float(len(t))\n",
    "    average_agg += (percent * len(t))/len(data)\n",
    "    \n",
    "    print(\"Cluster with label\",dict_max[i] + \" purity percentage :: \",str(percent))\n",
    "\n",
    "print(\"Average Purity is :: \",average_agg*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'K-Means Clustering', 'GMM', 'Agglomerative Clustering'\n",
    "input_list = [purity,avg_purity,average_agg]\n",
    "colors = ['red','blue','green']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    " \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.pie(input_list, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%f%%', shadow=True, startangle=180)\n",
    " \n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
